xlab("Carborators") +
ylab("Miles per Gallon")
ggplot(mtcars, aes(x=carb, y=mpg)) +
geom_area(aes(fill=factor(mpg), alpha=0.5)) +
scale_fill_brewer(palette="Dark2") +
xlab("Carborators") +
ylab("Miles per Gallon")
ggplot(mtcars, aes(x=carb, y=mpg)) +
geom_area(aes(fill=factor(mpg), alpha=0.5)) +
scale_fill_brewer(palette="Dark3") +
xlab("Carborators") +
ylab("Miles per Gallon")
ggplot(mtcars, aes(x=carb, y=mpg)) +
geom_area(aes(fill=factor(mpg), alpha=0.5)) +
scale_fill_brewer(palette="Dark") +
xlab("Carborators") +
ylab("Miles per Gallon")
ggplot(mtcars, aes(x=carb, y=mpg)) +
geom_area(aes(fill=factor(mpg), alpha=0.5)) +
scale_fill_brewer(palette="Dark2") +
xlab("Carborators") +
ylab("Miles per Gallon")
ggplot(mtcars, aes(x=carb, y=mpg)) +
geom_area(aes(fill=factor(mpg), alpha=0.5)) +
scale_fill_brewer(palette="Pastel") +
xlab("Carborators") +
ylab("Miles per Gallon")
ggplot(mtcars, aes(x=carb, y=mpg)) +
geom_area(aes(fill=factor(mpg), alpha=0.5)) +
scale_fill_brewer(palette="Pastel1") +
xlab("Carborators") +
ylab("Miles per Gallon")
ggplot(mtcars, aes(x=carb, y=mpg)) +
geom_area(aes(fill=factor(mpg), alpha=0.9)) +
scale_fill_brewer(palette="Pastel2") +
xlab("Carborators") +
ylab("Miles per Gallon")
ggplot(mtcars, aes(x=carb, y=mpg)) +
geom_area(aes(fill=factor(mpg)), alpha=0.)) +
ggplot(mtcars, aes(x=carb, y=mpg)) +
geom_area(aes(fill=factor(mpg)), alpha=0.5) +
scale_fill_brewer(palette="Pastel2") +
xlab("Carborators") +
ylab("Miles per Gallon")
ggplot(mtcars, aes(x=carb, y=mpg)) +
geom_area(aes(fill=factor(mpg)), alpha=0.8) +
scale_fill_brewer(palette="Pastel2") +
xlab("Carborators") +
ylab("Miles per Gallon")
###COMPLEX CHARTS – FACETED BARCHART
ggplot(mtcars, aes(x=factor(cyl))) +
geom_bar() +
facet_wrap(~gear) +
xlab("Number of Cylinders") +
ylab("Frequency")
ggplot(mtcars, aes(x=factor(cyl))) +
geom_bar() +
facet_wrap(~carb) +
xlab("Number of Cylinders") +
ylab("Frequency")
ggplot(mtcars, aes(x=factor(cyl))) +
geom_bar() +
facet_wrap(~qsec) +
xlab("Number of Cylinders") +
ylab("Frequency")
ggplot(mtcars, aes(x=factor(qsec))) +
geom_bar() +
facet_wrap(~cyl) +
xlab("Qsecs") +
ylab("Frequency")
ggplot(mtcars, aes(x=factor(carb))) +
geom_bar() +
facet_wrap(~cyl) +
xlab("Carb") +
ylab("Frequency")
ggplot(mtcars, aes(x=factor(hp))) +
geom_bar() +
facet_wrap(~cyl) +
xlab("Horsepower") +
ylab("Frequency")
ggplot(mtcars, aes(x=factor(carb))) +
geom_bar() +
facet_wrap(~cyl) +
xlab("Qsecs") +
ylab("Frequency")
###COMPLEX CHARTS – INTERACTIVE SCATTERPLOT
library(plotly)
fig <- plot_ly(mtcars, x = ~hp, y = ~mpg, type = 'scatter', mode = 'markers')
fig
###COMPLEX CHARTS – INTERACTIVE SCATTERPLOT
library(plotly)
figure_hp_mpg <- plot_ly(mtcars, x = ~hp, y = ~mpg, type = 'scatter', mode = 'markers')
figure_hp_mpg
fig <- plot_ly(mtcars, x = ~hp, y = ~mpg, type = 'scatter', mode = 'markers') |>
layout(title = "Customized Interactive Scatter Plot")
fig
###COMPLEX CHARTS – HEATMAPS
library(heatmaply)
heatmaply(cor(mtcars),
main = "Correlation Heatmap",
xlab = "Features",
ylab = "Features")
###COMPLEX CHARTS – HEATMAPS
library(heatmaply)
heatmaply(cor(mtcars),
main = "Correlation Heatmap",
xlab = "Features",
ylab = "Features")
library(heatmaply)
heatmaply(cor(mtcars),
main = "Correlation Heatmap",
xlab = "Features",
ylab = "Features",
colors = heat.colors(256))
library(heatmaply)
heatmaply(cor(mtcars),
main = "Correlation Heatmap",
xlab = "Features",
ylab = "Features",
colors = heat.colors(257))
library(heatmaply)
heatmaply(cor(mtcars),
main = "Correlation Heatmap",
xlab = "Features",
ylab = "Features",
colors = heat.colors(256))
###COMPLEX CHARTS – 3D SCATTER PLOTS
library(scatterplot3d)
scatterplot3d(mtcars$hp, mtcars$mpg, mtcars$wt,
main="3D Scatter Plot")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(janitor)
#setwd("~/Documents/GitHub/congressional-private-travel")
sponsor_df <- read_csv("../../standardize_sponsors_dests/standardized_sponsors/open_refined_sponsors.csv")
#Looking at the new data set from the open refine list. Standardized but could still be cleaned
unique_sponsors <- sponsor_df  |>
group_by(sponsors_cleaned) |>
summarise()
unique_sponsors
write_csv(unique_sponsors, 'unique_sponsors.csv')
view(unique_sponsors)
# Loading the updated and original unstandardized list to compare
updated_unstandardized_unique_sponsors <- read_csv("updated_unstandardized_unique_sponsors.csv")
og_unstandardized_unique_sponsors <- read_csv("unstandardized_unique_sponsors.csv")
# Checking for any new sponsors in the recent months following the update
updated_rows <- setdiff(og_unstandardized_unique_sponsors, updated_unstandardized_unique_sponsors)
updated_rows
stand_dests <- read_csv("../standardizing_dests/house_12to23_w_stand_dests.csv")
joined_spons_dests <- stand_dests |> left_join(sponsor_df, by = "TravelSponsor")
joined_spons_dests
solo_sponsors <- joined_spons_dests |>
select(DocID, TravelSponsor, sponsors_cleaned)
#Steps left:
#Add number of sponsors column -> mutate(number_of_sponsors = ...)
#Write code to fill out that column
solo_sponsors <- solo_sponsors |>
mutate(sponsors_cleaned = (str_remove_all(sponsors_cleaned, ", Inc.")))
solo_sponsors <- solo_sponsors |>
mutate(sponsors_cleaned = (str_remove_all(sponsors_cleaned, ", Inc")))
solo_sponsors <- solo_sponsors |>
mutate(number_sponsors = case_when(
str_count(sponsors_cleaned, "[;,/]|And The|And") == 0 ~ 1,
str_count(sponsors_cleaned, "[;,/]|And The|And") == 1 ~ 2,
str_count(sponsors_cleaned, "[;,/]|And The|And") == 2 ~ 3,
str_count(sponsors_cleaned, "[;,/]|And The|And") == 3 ~ 4,
str_count(sponsors_cleaned, "[;,/]|And The|And") == 4 ~ 5,
# Add more conditions as needed for higher counts
TRUE ~ NA_integer_))
#need to clean the sponsors data - right now it's counting a bunch of stuff that is one sponsor as two because we included "and", as well as counting things that have nothing in between two sponsors (eg. "humpty dumpty instituteand bill and melinda gates") - can look at all of them that have more than 1 sponsor and clean that way
#these are the problem children I found that need further standardizing:
#International Conservative Cacus Foundationhalo Trust
#Savannah River Site Community Reuse Organization (srscro) and Srs Community Reuse Organization (srscro)
#The National Alliance On Mental Illness - Nys and National Alliance On Mental Health Illness - New York State
#Robert Wood Johnson Foundation (rwjf) Center For Health Policy At The University Of New Mexico and Rwjf Center For Health Policy At The University Of New Mexico to and Center For Health Policy, University Of New Mexico
View(joined_spons_dests)
View(solo_sponsors)
library(tidyverse)
library(janitor)
library(lubridate)
###asked chatgpt how to turn a character column into a date time column, gave it some examples from our code and int told me to do this:
wh_visitor_data <- read_csv("data/combined.csv") |> clean_names() |>
mutate(toa = mdy_hm(toa)) |>
mutate(tod = mdy_hm(tod)) |>
mutate(appt_made_date = mdy_hm(appt_made_date))  |>
mutate(appt_start_date = mdy(appt_start_date))  |>
mutate(appt_end_date = mdy_hm(appt_end_date))  |>
mutate(lastentrydate = mdy_hm(lastentrydate))  |>
mutate(releasedate = mdy(releasedate))
glimpse(wh_visitor_data)
# We want a clean single dataframe to work with. Right now there are a lot of columns that might not be particularly interesting or useful. For example, let's get rid of x28 and x29, caller room, appt cancel date, terminal suffix, poa, tod, pod,  meeting room, and post.
clean_wh_visitor_data <- wh_visitor_data |>
select(-poa, -access_type, -pod, -appt_cancel_date, -caller_room, -uin, -bdgnbr, -last_updatedby, -post, -terminal_suffix,-caller_name_last, -caller_name_first)
# Full transparency, I couldn't remember the code for deleting a column so I asked ChatGPT the following: "using the tidyverse, janitor, and lubridate libraries in R, how do I delete a column?"
###note - we were having problems with this so I asked chatgpt: how do I make a new column with just the date from a column that includes date and time using lubridate and the tidyverse? it told me to do mutate(toa_date = ymd_hms(toa) %>% date())
clean_wh_visitor_data <- clean_wh_visitor_data |>
mutate(toa_date = ymd_hms(toa) |> date()) |>
mutate(tod_date = ymd_hms(tod) |>date())
####mutate(toa_time = ymd_hms(toa) |> time()) we need to figure out how to separate out time from these two columns as well
clean_wh_visitor_data |>
group_by(toa_date) |>
summarize(count=n()) |>
arrange(desc(count))
# note - since there are so many NA dates here, I think we should group by appt date.
wh_visitors_by_date <- clean_wh_visitor_data |>
group_by(appt_start_date) |>
summarize(total_visits = n()) |>
arrange(desc(total_visits))
### just kidding, there are a ton of NA dates still. maybe appt end date?
wh_visitors_by_end_date <- clean_wh_visitor_data |>
group_by(appt_end_date) |>
summarize(total_visits = n()) |>
arrange(desc(total_visits))
### this seems better
###derek asked if the NA dates were specific to certain offices. how do we figure that out?
clean_wh_visitor_data |> filter(if_any(-toa_date, is.na)) |>
group_by(meeting_loc) |>
summarise(count = n())
###mutate(toa_time = ymd_hms(toa) |> time()) we need to figure out how to separate out time from these two columns as well
clean_wh_visitor_data |>
group_by(toa_date) |>
summarize(count=n()) |>
arrange(desc(count))
# note - since there are so many NA dates here, I think we should group by appt date.
wh_visitors_by_date <- clean_wh_visitor_data |>
group_by(appt_start_date) |>
summarize(total_visits = n()) |>
arrange(desc(total_visits))
### just kidding, there are a ton of NA dates still. maybe appt end date?
wh_visitors_by_end_date <- clean_wh_visitor_data |>
group_by(appt_end_date) |>
summarize(total_visits = n()) |>
arrange(desc(total_visits))
# July 2 calculation
wh_visitor_data <- wh_visitor_data |>
mutate(as_datetime(mdy_hm(appt_end_date)))
wh_visitor_data |>
mutate(date = as_date(as_datetime(mdy_hm(appt_end_date)))) |>
filter(str_detect(date, "2023-07-02")) |>
group_by(date) |>
summarize(sum= sum(total_people))
### this seems better
clean_wh_visitor_data |>
group_by(appt_end_date) |>
filter(total_people <100) |>
summarize(count=n()) |>
arrange(desc(count))
### Note - asked ChatGPT how to make the month column have labels and gave it my mutate code creating a new col with lubridate
#reading in approval ratings data from gallup
approval_ratings <- read_csv("data/biden_approval_gallup.csv", col_names = FALSE)
colnames(approval_ratings) <- c("poll_date", "rep_approval_pct", "ind_approval_pct", "dem_approval_pct")
approval_ratings <- approval_ratings |>
separate(poll_date, into = c("year", "month", sep = "-")) |>
select(year, month, rep_approval_pct, ind_approval_pct, dem_approval_pct)
approval_ratings <- approval_ratings |>
mutate(date = paste(year, month, sep = " "))
#making a df with top months for visitors
wh_visitor_by_month <- wh_visitors_by_end_date |>
mutate(date_time = ymd_hms(appt_end_date),
month = month.abb[month(appt_end_date)])
wh_visitor_by_month <- wh_visitor_by_month |>
group_by(month) |>
summarise(total_visits = sum(total_visits))
#last one not really helpful bc didn't sort by year. Now doing the same with year and month
wh_visitors_month_year <- wh_visitors_by_end_date |>
mutate(date_time = ymd_hms(appt_end_date),
month = month.abb[month(appt_end_date)],
year = year(date_time))
top_months_by_year <- wh_visitors_month_year |>
group_by(year, month) |>
summarise(total_visits = sum(total_visits))
#not sure if that was useful. I think I need to join these dataframes. Asked chatgpt for help with this because I needed to join on the date column and didn't have one with year and month in it for top_months_by_year. Asked it: I want to create a date column that looks like this: 2022 Jan, 2022 Feb, 2022 March. Currently I have a year column that looks like this: 2022, 2022, 2022, and a month column that looks like this: Jan, Feb, March. How can I create this new column using r and the tidyverse?
top_months_by_year <- top_months_by_year |>
mutate(date = as.Date(paste(year, month, "01"), format = "%Y %b %d"))
approval_ratings <- approval_ratings |>
mutate(date = as.Date(paste(year, month, "01"), format = "%Y %b %d"))
visits_by_month_ratings <- top_months_by_year |>
left_join(approval_ratings, by = "date") |>
rename(year = year.x, month = month.x) |>
select(-year.y, -month.y) |>
na.omit(visits_by_month_ratings)
#note - chatgpt wrote this code for me, I wanted to put these on the same graph but couldn't figure out the scale part
library(ggplot2)
ggplot(visits_by_month_ratings, aes(x = date)) +
# Bar chart for total_visits
geom_bar(aes(y = total_visits), stat = "identity", fill = "darkgray") +
# Line graphs for approval_ratings
geom_line(aes(y = dem_approval_pct * max(total_visits) / max(dem_approval_pct)), color = "blue") +
geom_line(aes(y = ind_approval_pct * max(total_visits) / max(dem_approval_pct)), color = "darkgreen") +
geom_line(aes(y = rep_approval_pct * max(total_visits) / max(dem_approval_pct)), color = "red") +
# Y-axis scale for total_visits
scale_y_continuous(name = "Total Visits", breaks = seq(0, 70000, by = 10000)) +
# Secondary Y-axis scale for approval_rating
scale_y_continuous(
name = "Total Visits",
sec.axis = sec_axis(~ . * max(visits_by_month_ratings$dem_approval_pct) / max(visits_by_month_ratings$total_visits), name = "Dem Approval Rating")
) +
theme_gray()
#note - not sure if this is actually a valuable graph. Should we just throw out 2021 data because it's so sparse compared to 2022/23? Does this highlight fluctuations in visits or just a lack of data for earlier months? Also it wouldn't graph Dec 2022 which was the second highest month because of NA values in the approval ratings. @DEREK is there a way around this?
open_refine_cleaned_visitors <- read_csv("data/open_refined_wh_data_combined.csv")
open_refine_cleaned_visitors |> group_by(VISITEE_NAMELAST_CLEAN, VISITEE_NAMEFIRST_CLEAN) |>
summarise(visits = n()) |>
arrange(desc(visits))
wh_visitor_data %>%
filter(str_detect(visitee_namelast, regex("office", ignore_case = TRUE))) |>
filter(str_detect(visitee_namefirst, regex("visitors", ignore_case = TRUE))) |>
summarize(count = n())
# result - 462349 entries with just that office visitors bs
462349/847371
clean_wh_visitor_data |>
filter(
str_detect(namelast, regex("altman", ignore_case = TRUE))
&
str_detect(namefirst, regex("^sam", ignore_case=TRUE))
)
View(open_refine_cleaned_visitors)
---
title: "R Notebook"
library(tidyverse)
library(ggplot2)
library(plotly)
library(janitor)
library(tidycensus)
install.packages("stringdist")
library(stringdist)
setwd("~/data_journalism/taylor-nichols-JOUR652/final_data_project")
louisiana_prison_sentences <- read_csv("lousiana_life_without_parole_sept_2021.csv") |> clean_names()
library(tidyverse)
library(ggplot2)
library(plotly)
library(janitor)
library(tidycensus)
install.packages("stringdist")
library(stringdist)
setwd("~/data_journalism/taylor-nichols-JOUR652/final_data_project")
louisiana_prison_sentences <- read_csv("lousiana_life_without_parole_sept_2021.csv") |> clean_names()
install.packages("stringdist")
library(tidyverse)
library(ggplot2)
library(plotly)
library(janitor)
library(tidycensus)
install.packages("stringdist")
library(stringdist)
setwd("~/data_journalism/taylor-nichols-JOUR652/final_data_project")
louisiana_prison_sentences <- read_csv("lousiana_life_without_parole_sept_2021.csv") |> clean_names()
library(tidyverse)
library(ggplot2)
library(plotly)
library(janitor)
library(tidycensus)
install.packages("stringdist")
library(stringdist)
setwd("~/data_journalism/taylor-nichols-JOUR652/final_data_project")
louisiana_prison_sentences <- read_csv("lousiana_life_without_parole_sept_2021.csv") |> clean_names()
install.packages("stringdist")
library(tidyverse)
library(ggplot2)
library(plotly)
library(janitor)
library(tidycensus)
install.packages("stringdist")
library(stringdist)
setwd("~/data_journalism/taylor-nichols-JOUR652/final_data_project")
louisiana_prison_sentences <- read_csv("lousiana_life_without_parole_sept_2021.csv") |> clean_names()
setwd("~/data_journalism/taylor-nichols-JOUR652/final_data_project/data")
library(tidyverse)
library(ggplot2)
library(plotly)
library(janitor)
library(tidycensus)
install.packages("stringdist")
library(stringdist)
setwd("~/data_journalism/taylor-nichols-JOUR652/final_data_project")
louisiana_prison_sentences <- read_csv("lousiana_life_without_parole_sept_2021.csv") |> clean_names()
install.packages("stringdist")
library(tidyverse)
library(ggplot2)
library(plotly)
library(janitor)
library(tidycensus)
install.packages("stringdist")
library(stringdist)
setwd("~/data_journalism/taylor-nichols-JOUR652/final_data_project")
louisiana_prison_sentences <- read_csv("lousiana_life_without_parole_sept_2021.csv") |> clean_names()
install.packages("stringdist")
library(tidyverse)
library(ggplot2)
library(plotly)
library(janitor)
library(tidycensus)
library(stringdist)
setwd("~/data_journalism/taylor-nichols-JOUR652/final_data_project")
louisiana_prison_sentences <- read_csv("lousiana_life_without_parole_sept_2021.csv") |> clean_names()
setwd("~/data_journalism/taylor-nichols-JOUR652/final_data_project/data")
library(tidyverse)
library(ggplot2)
library(plotly)
library(janitor)
library(tidycensus)
library(stringdist)
louisiana_prison_sentences <- read_csv("lousiana_life_without_parole_sept_2021.csv") |> clean_names()
ggplot(louisiana_prison_sentences, aes(x=factor(bucket_age))) +
geom_bar() +
geom_text(aes(label = after_stat(count)), stat = "count", vjust = -.2, colour = "black") +
xlab("Bucket Age") +
ylab("Number of Incarcerated People") +
theme_linedraw()
ggplot(louisiana_prison_sentences, aes(x=factor(offenses))) +
geom_bar() +
xlab("Type of Offense") +
ylab("Number of Incarcerated People") +
theme_linedraw()
fig <- plot_ly(louisiana_prison_sentences, x = ~offenses, y = ~bucket_age, type = 'scatter', mode = 'markers')
fig
offenses <- louisiana_prison_sentences |> group_by(offenses) |>
summarise(total = n())
write_csv(offenses, "~/data_journalism/taylor-nichols-JOUR652/final_data_project/offenses.csv")
offenses_age <- louisiana_prison_sentences |> group_by(bucket_age, offenses) |>
summarise(total = n()) |>
arrange(desc(total))
write_csv(offenses_age, "~/data_journalism/taylor-nichols-JOUR652/final_data_project/offenses_age.csv")
age <- louisiana_prison_sentences |> group_by(bucket_age) |>
summarise(total = n()) |>
arrange(desc(total))
write_csv(age, "~/data_journalism/taylor-nichols-JOUR652/final_data_project/bucket_age.csv")
offenses_top_15 <- offenses |> top_n(15) |>
arrange(desc(total))
write_csv(offenses_top_15, "~/data_journalism/taylor-nichols-JOUR652/final_data_project/offenses_top_15.csv")
#lets load census data
acs_variables <- load_variables(2021, "acs5", cache = TRUE)
#making a data frame of racial and gender makeup of sentences
la_prison_sentences_race <- louisiana_prison_sentences |>
group_by(race, gender) |>
summarise (total_offenses = n())
louisiana_prison_sentences_parishes <- louisiana_prison_sentences |>
group_by(parish_of_conviction, race) |>
summarise(total_convictions = n())
#pulling racial demographic data for louisiana
la_racial_demographics <- get_acs(geography = "county",
variables = c(total_race = "B02001_001", white = "B02001_002", black = "B02001_003"),
state = "LA",
year = 2021)
#now lets fix our demographics dataframe
la_racial_demographics_clean <- la_racial_demographics |> clean_names() |>
pivot_wider(names_from = variable, values_from = estimate)
#more fixing and calculating percentages
la_racial_demographics_clean_2 <- la_racial_demographics_clean |> group_by(geoid) |>
fill(total_race, white, black) |>
filter(!is.na(total_race) & !is.na(white) & !is.na(black)) |>
select(-moe) |>
mutate(pct_white = white/total_race*100, pct_black = black/total_race*100)
#now lets look at the parish data before we join
louisiana_prison_sentences_parishes <- louisiana_prison_sentences_parishes |>
group_by(parish_of_conviction) |>
mutate(total_parish = sum(total_convictions, na.rm = TRUE)) |>
ungroup()
louisiana_prison_sentences_parishes <- louisiana_prison_sentences_parishes |>
mutate(pct_of_total = total_convictions/total_parish*100)
#ok let's try and join
# Function to find the best match
la_parishes_demographics <- louisiana_prison_sentences_parishes %>%
mutate(matched_parish = sapply(parish_of_conviction, function(x) la_racial_demographics_clean_2$name[str_detect(tolower(la_racial_demographics_clean_2$name), tolower(x))][1])) %>%
left_join(la_racial_demographics_clean_2, by = c("matched_parish" = "name"))
# Clean up and remove intermediate columns
la_parishes_demographics <- la_parishes_demographics %>%
select(-matched_parish)
write.csv(la_parishes_demographics, "~/data_journalism/taylor-nichols-JOUR652/final_data_project/parishes_demographics.csv")
top_offenses_age <- louisiana_prison_sentences |>
group_by(bucket_age, offenses) |>
summarize(total_offenses = n())
write_csv(top_offenses_age, "~/data_journalism/taylor-nichols-JOUR652/final_data_project/top_offenses_age.csv")
louisiana_prison_sentences |>
group_by(offenses, bucket_age) |>
summarise(count = n()) |>
arrange(desc(bucket_age))5 |>
sentences <- louisiana_prison_sentences |>
group_by(sentences) |>
summarise(total_sentences = n())
View(la_parishes_demographics)
View(la_prison_sentences_race)
View(la_racial_demographics)
View(la_racial_demographics_clean)
View(la_racial_demographics_clean_2)
View(louisiana_prison_sentences)
sentences <- louisiana_prison_sentences |>
group_by(offenses) |>
summarise(total_sentences = n())
sentences <- louisiana_prison_sentences |>
group_by(offenses) |>
summarise(total_sentences = n())
write_csv(sentences, "sentences.scv")
sentences <- louisiana_prison_sentences |>
group_by(offenses) |>
summarise(total_sentences = n())
write_csv(sentences, "sentences.csv")
sentences <- louisiana_prison_sentences |>
group_by(offenses) |>
summarise(total_sentences = n()) |>
top(10)
sentences <- louisiana_prison_sentences |>
group_by(offenses) |>
summarise(total_sentences = n()) |>
arrange(desc(total_sentences)) |>
slice(1:10)
sentences <- louisiana_prison_sentences |>
group_by(offenses) |>
summarise(total_sentences = n()) |>
arrange(desc(total_sentences)) |>
slice(1:10)
write_csv(sentences, "top10_sentences.csv")
